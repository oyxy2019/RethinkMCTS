# -*- coding:utf-8 _*-
from torch.utils.data import DataLoader, SequentialSampler
from utils import *
import time
from models import *
# from torcheval.metrics import HitRate, ReciprocalRank
# import torchmetrics
from dataSet import *
from tqdm import tqdm
from math import sqrt, log
from executors import AppsExecutor, HumanevalExecutor
from ChatModels import GPTChat

class BS:
    def __init__(self, args):
        self.args = args
        self.sample_times = 0
        self.gamma = 0.9
        if 'gpt3.5' in args.arch or 'gpt4' in args.arch:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
            self.generator = GPTChat(args.arch, self.tokenizer, args)
        else:
            raise ValueError('wrong chat model')

        if args.dataset == 'apps':
            self.executor = AppsExecutor(args)
        elif args.dataset == 'humaneval':
            self.executor = HumanevalExecutor(args)

    def generate(self, problem_instance):
        st = time.time()
        self.cur_prob_instance = problem_instance
        raw_prompt = problem_instance['prompt']
        initial_state = self.tokenizer.encode(raw_prompt)
        if len(initial_state) >= self.args.horizon:
            return None

        state = self.generator.get_token_predict_sequence(initial_state)
        complete_programs_ids = [state]
        complete_programs = [self.convert_state_to_program(state)]

        train_rewards = [0.0]
        test_rewards = [self.get_reward(s) for s in complete_programs_ids]

        output_dict = {}
        output_dict['final_program'] = complete_programs[0]
        output_dict['train_reward'] = train_rewards[0]
        output_dict['test_reward'] = test_rewards[0]

        output_dict['all_programs'] = complete_programs
        output_dict['all_train_rewards'] = train_rewards
        output_dict['all_test_rewards'] = test_rewards
        output_dict['avg_sample_time'] = time.time() - st

        return output_dict

    def get_reward(self, s):
        output_str = self.convert_state_to_program(s)
        try:
            curr_res = self.executor.check_correctness(self.cur_prob_instance, output_str, 'test')
            fixed = []
            for e in curr_res:
                if isinstance(e, np.ndarray):
                    e = e.item(0)
                if isinstance(e, np.bool_):
                    e = bool(e)
                fixed.append(e)
            curr_res = fixed
            # if not np.all(curr_res):
            #     print(f"Results were not all True: {curr_res}")
        except Exception as e:
            print(f"test framework exception = {repr(e)}{e}\n")
            curr_res = []
        # How to read results [-2] = compile error, [-1] = runtime error [False] = failed test case [True] = passed test case")
        assert isinstance(curr_res, list)
        pass_rate = np.mean(np.asarray(curr_res) > 0) if len(curr_res) > 0 else 0
        reward = pass_rate
        return reward

    def convert_state_to_program(self, s):
        s = self.tokenizer.decode(s)
        if "ANSWER:" in s:
            s = s.split("ANSWER:\n")[1]
        s = s.replace("<|endoftext|>", "")
        return s